[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Christel Lacaze Swift",
    "section": "",
    "text": "Sharing some approaches I’m using in my current work which is mainly to do with media measurement.\nAll coded examples are written in R."
  },
  {
    "objectID": "posts/cross_media_with_copulas/index.html",
    "href": "posts/cross_media_with_copulas/index.html",
    "title": "Estimating Cross Media Usage Using Copulas",
    "section": "",
    "text": "Bringing separate datasets together is a common issue in media measurement because traditionally each medium is measured by a different system: in the UK, TV is measured by Barb, radio by RAJAR, published media by Pamco, outdoor media by Route etc… These separate media studies are called “currencies” because they are governed by joint industry committees with representatives from both the buying and selling side, and are therefore accepted as the trading system for each medium.\nAdvertising campaigns on each medium are planned, bought and traded using these separate studies. However there is a need to understand how many people have been reached and how many times across all media for each campaign. For media owners present across more than one media type, they also want to be able to measure their combined audience regardless of media.\nThis is mainly done using data fusion (see Ipsos’s white paper (Sharot 2011)) where the best statistical matches are found across studies and brought together to form one fully granular dataset. A prominent example of this is the IPA’s Touchpoints study. To find good matches between respondents from the different datasets, a good set of explanatory common variables is crucial. Data fusion relies on the assumption of conditional independence, meaning that for example we assume that tv viewing behaviour can be fully explain by these common hooks. In practice this is rarely the case and the remaining unexplained variance leads to what is commonly called “regression to the mean”: the interaction between two fused variables is never as strong as it would be if they were coming from the same study, and it ends up somewhere between the truth and what it would be in the case of independence (i.e. one variable has no association with the other). However this is seen as an acceptable trade-off as the fused datasets allow full flexibility in data analysis.\nIn this article, we explore another way to bring separate univariate distributions together using copulas, a very flexible statistical technique for multivariate modelling. With the help of reproducible examples, we demonstrate the process of recreating a simulated dataset from the observed correlation and marginal distributions of a real dataset. The use of the Tetrachoric correlation is particularly well-suited for the case of zero-inflated distributions when the main objective is to preserve the combined reach."
  },
  {
    "objectID": "posts/cross_media_with_copulas/index.html#overview",
    "href": "posts/cross_media_with_copulas/index.html#overview",
    "title": "Estimating Cross Media Usage Using Copulas",
    "section": "2.1 overview",
    "text": "2.1 overview\nCopulas are used to model the dependence structure of multivariate distributions. They were introduced by Abe Sklar (Sklar 1959) in 1959 and are particularly popular in quantitative finance for risk management and portfolio optimisation.\nA copula takes as input random variables \\(U_i\\) that are uniformly distributed on \\([0,1]\\) and it will then output their joint cumulative distribution: given the uniforms \\(U_1, ...,U_k\\) a copula \\(C\\) would output \\(C(u_1,... u_k) = P(U_1 \\leq u_1, ...,U_k \\leq u_k)\\).\nThe property called the Probability Integral Transform, also known as the Universality of the Uniform, states that any continuous distribution can be converted to a standard uniform distribution and vice-versa: if \\(X\\) is a continuous variable with cumulative distribution \\(F_X\\), then the random variable \\(Y := F_X(X)\\) has a standard uniform distribution.\nUsing this property we can then map the uniform margins from our copula to any continuous distribution using their quantile function.\nWorking with copulas means that we can separate the problems of modelling the margins and modelling the dependence structure. These tow steps can be done independently.\nIn practice, a possible multivariate modelling workflow could be as follows:\n- given a multivariate distribution, take their margins and find the best univariate model for each one.\n- transform each margin by taking their pseudo-observations, i.e. their ranks: this will make each margin uniformly distributed on \\([0,1]\\).\n- find a good copula that fits the resulting dependence structure.\nThen to produce a random sample from this modelled distribution:\n- create a random sample from the chosen copula: each margin will be \\(U[0,1]\\) but they will be linked by the chosen dependence structure.\n- transform each margin to the chosen univariate distributions: the dependence structure will be preserved."
  },
  {
    "objectID": "posts/cross_media_with_copulas/index.html#some-examples-of-copulas",
    "href": "posts/cross_media_with_copulas/index.html#some-examples-of-copulas",
    "title": "Estimating Cross Media Usage Using Copulas",
    "section": "2.2 some examples of copulas",
    "text": "2.2 some examples of copulas\nThe main families of copulas are the elliptical (e.g. Gaussian and Student-t) and Archimedean copulas (e.g. Clayton, Frank, Gumbel, Joe).\nDifferent copulas allow for different dependence shapes, including asymmetrical, and different emphasis on the tails.\n\n\n\nexamples of copulas\n\n\nThe Archimedean copulas only take one parameter which means they can only be used when we assume the same dependence between all pairs of variables.\nThe Gaussian copula is based on a correlation matrix where each pair of variables can have their own value. The same applies to the Student-t with an added parameter \\(\\nu\\) for the degrees of freedom which controls the tail dependence: lower values correspond to heavier tails, so that extreme joint events are more likely. The Student-t copula approaches the Gaussian as \\(\\nu\\) increases."
  },
  {
    "objectID": "posts/cross_media_with_copulas/index.html#walk-through-with-a-simulated-example",
    "href": "posts/cross_media_with_copulas/index.html#walk-through-with-a-simulated-example",
    "title": "Estimating Cross Media Usage Using Copulas",
    "section": "2.3 walk through with a simulated example",
    "text": "2.3 walk through with a simulated example\nHere we show how to go from a multivariate normal distribution with a given correlation matrix to a multivariate distribution with the same correlation but with different margins: a Gamma, a Beta and a Student-t.\nThe multivariate normal distribution (MVN) is essentially a Gaussian copula with Gaussian margins. It is easy to generate a random sample from a MVN from most statistical tools, and it can even be done in excel. In R, we can use MASS::mvrnorm. More complex copulas can be found in the dedicated copula package (see (Yan 2007)).\n\nlibrary(MASS) # to use the multivariate normal distribution mvrnorm\n# make sure to load tidyverse after MASS so that the select function comes from dplyr\nlibrary(tidyverse) \nlibrary(psych) # for the pairs.panels and phi2tetra functions\n\noptions(dplyr.summarise.inform = FALSE) # to suppress annoying grouping warning\ntheme_set(theme_classic()) # sets the ggplot theme for the session\n\n# specify a correlation matrix:\nmy_cor &lt;- matrix(\n  data = c(1, 0.4, 0.2,\n           0.4, 1, -0.8,\n           0.2, -0.8, 1),\n  byrow = TRUE,\n  nrow = 3\n)\n\nset.seed(2025)\nn = 10000\n\n# produce a random sample of 10k units \n# on 3 normal margins N(0,1) with the specified correlation:\nmy_sim &lt;- MASS::mvrnorm(\n  n = n,\n  mu = c(0,0,0),\n  Sigma = my_cor\n)\n\nsummary(my_sim)\n\n       V1                  V2                  V3            \n Min.   :-3.886273   Min.   :-4.134011   Min.   :-3.8664468  \n 1st Qu.:-0.688413   1st Qu.:-0.678848   1st Qu.:-0.6775139  \n Median :-0.008261   Median :-0.011869   Median : 0.0103952  \n Mean   :-0.012727   Mean   :-0.006768   Mean   :-0.0003453  \n 3rd Qu.: 0.663353   3rd Qu.: 0.687833   3rd Qu.: 0.6599113  \n Max.   : 4.195759   Max.   : 3.307045   Max.   : 3.8395707  \n\n\n\ncor(my_sim)\n\n          [,1]       [,2]       [,3]\n[1,] 1.0000000  0.4021567  0.1970059\n[2,] 0.4021567  1.0000000 -0.8000375\n[3,] 0.1970059 -0.8000375  1.0000000\n\n\n\npairs.panels(my_sim, cex.cor=0.6)\n\n\n\n\n\n\n\n\nThe first step is to take their pseudo-observations, or ranks, to turn each margin into a standard uniform \\(U(0,1)\\):\n\nmy_sim_pseudo_obs &lt;- my_sim %&gt;% \n  as.data.frame() %&gt;% \n  # add the original row numbering so we can use it later:\n  mutate(rownum = row_number()) %&gt;% \n  pivot_longer(V1:V3, names_to = \"variable\") %&gt;% \n  group_by(variable) %&gt;% \n  # divide the rank by n+1 to ensure the result is &lt; 1:\n  mutate(pseudo_obs = rank(value)/ (n+1)) %&gt;% \n  select(-value) %&gt;% \n  pivot_wider(names_from = variable, values_from = pseudo_obs)\n\nhead(my_sim_pseudo_obs)\n\n# A tibble: 6 × 4\n  rownum    V1    V2    V3\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1 0.154 0.190 0.491\n2      2 0.640 0.433 0.493\n3      3 0.120 0.171 0.570\n4      4 0.853 0.124 0.951\n5      5 0.618 0.424 0.743\n6      6 0.823 0.596 0.571\n\n\n\nsummary(my_sim_pseudo_obs)\n\n     rownum            V1                   V2                   V3            \n Min.   :    1   Min.   :0.00009999   Min.   :0.00009999   Min.   :0.00009999  \n 1st Qu.: 2501   1st Qu.:0.25005000   1st Qu.:0.25005000   1st Qu.:0.25005000  \n Median : 5000   Median :0.50000000   Median :0.50000000   Median :0.50000000  \n Mean   : 5000   Mean   :0.50000000   Mean   :0.50000000   Mean   :0.50000000  \n 3rd Qu.: 7500   3rd Qu.:0.74995000   3rd Qu.:0.74995000   3rd Qu.:0.74995000  \n Max.   :10000   Max.   :0.99990001   Max.   :0.99990001   Max.   :0.99990001  \n\n\nThe pseudo-observations are all standard uniforms.\n\nmy_sim_pseudo_obs %&gt;% \n  select(-rownum) %&gt;% \n  pairs.panels(cex.cor=0.6)\n\n\n\n\n\n\n\n\nBy using the pseudo-observations of each margin, they have become \\(U(0,1)\\) but the correlation is still the same. The empirical cumulative distribution of this multivariate sample is our empirical copula: it has a Gaussian dependence structure with standard uniform margins.\nNote that in this case, as we know that our margins were originally \\(N(0,1)\\), we could have used their distribution function pnorm to turn them into uniforms:\n\nmy_sim_p &lt;- pnorm(my_sim)\n\npairs.panels(my_sim_p, method = \"spearman\", cex.cor=0.6)\n\n\n\n\n\n\n\n\nBy taking the pseudo-observations of the margins, we’re left with just the dependence structure.\nNow that we have uniform margins we can transform them into any continuous distribution, for example a Gamma, a Beta and a Student t using their quantile functions qgamma, qbeta and qt (i.e. inverse CDFs):\n\nmy_sim_new &lt;- my_sim_pseudo_obs %&gt;% \n  pivot_longer(V1:V3, names_to = \"variable\", values_to = \"x\") %&gt;% \n  mutate(\n    y = case_when(\n      variable == \"V1\" ~ qgamma(x, shape = 2, scale = 1),\n      variable == \"V2\" ~ qbeta(x, shape1 = 3, shape2 = 2),\n      variable == \"V3\" ~ qt(x, df = 5)\n    )\n  ) %&gt;% \n  select(-x) %&gt;% \n  pivot_wider(names_from = variable, values_from = y)\n\nmy_sim_new %&gt;% \n  select(-rownum) %&gt;% \n  # use the Spearman correlation as it is based on ranks:\n  pairs.panels(method = \"spearman\", cex.cor=0.6)\n\n\n\n\n\n\n\n\nWe started with a multivariate normal and we now have a multivariate distribution made of a Gamma, a Beta and a Student t margins but the transformation preserved the dependence structure.\nNote that when working with copulas, and in particular when comparing correlations, we use a rank-based correlation like Spearman’s rho or Kendall’s tau rather than the Pearson correlation. As the Spearman and Kendall coefficients are based on the ranks rather than the actual values, they will not be distorted by skewed or asymmetrical margins."
  },
  {
    "objectID": "posts/cross_media_with_copulas/index.html#non-continuous-distributions",
    "href": "posts/cross_media_with_copulas/index.html#non-continuous-distributions",
    "title": "Estimating Cross Media Usage Using Copulas",
    "section": "2.4 non-continuous distributions",
    "text": "2.4 non-continuous distributions\nThe Sklar theorem states that for any multivariate distribution with continuous margins, there exists a unique copula \\(C\\) such that:\n\\[F(x_1, x_2, \\dots, x_d) = C(F_1(x_1), F_2(x_2), \\dots, F_d(x_d))\\] where:\n- \\(F\\) is the joint CDF,\n- \\(F_i\\) are the marginal CDFs,\n- \\(C\\) is the copula.\nHowever when the margins are non-continuous, this theorem no longer holds and the uniqueness is not guaranteed, see (Nešlehová 2007).\nFrom a practical point, when margins are not continuous, they can no longer be mapped to uniform margins. For example, in the case of a zero-inflated distribution, we would also end up with a spike at zero when taking the pseudo-observations: many observations would have the same rank. In turn, this would mean we would no longer be able to uniquely transform the pseudo-values using the inverse CDF."
  },
  {
    "objectID": "posts/cross_media_with_copulas/index.html#example-using-movielense-dataset",
    "href": "posts/cross_media_with_copulas/index.html#example-using-movielense-dataset",
    "title": "Estimating Cross Media Usage Using Copulas",
    "section": "3.1 example using MovieLense dataset",
    "text": "3.1 example using MovieLense dataset\nTo provide a reproducible example for the discrete case, we can use the MovieLense dataset that is made available in the R package recommenderlab. It contains about 100,000 ratings from 943 users on 1664 movies. Movie genres are also made available in the MovieLenseMeta dataset so we can derive a multivariate dataset showing the number of items each user has watched from each genre.\n\n3.1.1 preparing the dataset\n\nlibrary(recommenderlab) # to use the MovieLense dataset\nlibrary(janitor) # to use the clean_names function\n\ndata(\"MovieLense\")\nstr(MovieLense)\n\nFormal class 'realRatingMatrix' [package \"recommenderlab\"] with 2 slots\n  ..@ data     :Formal class 'dgCMatrix' [package \"Matrix\"] with 6 slots\n  .. .. ..@ i       : int [1:99392] 0 1 4 5 9 12 14 15 16 17 ...\n  .. .. ..@ p       : int [1:1665] 0 452 583 673 882 968 994 1386 1605 1904 ...\n  .. .. ..@ Dim     : int [1:2] 943 1664\n  .. .. ..@ Dimnames:List of 2\n  .. .. .. ..$ : chr [1:943] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. .. ..$ : chr [1:1664] \"Toy Story (1995)\" \"GoldenEye (1995)\" \"Four Rooms (1995)\" \"Get Shorty (1995)\" ...\n  .. .. ..@ x       : num [1:99392] 5 4 4 4 4 3 1 5 4 5 ...\n  .. .. ..@ factors : list()\n  ..@ normalize: NULL\n\n\nThe data is stored as a realRatingMatrix object so we first turn it into a dataframe:\n\nfilms_rld &lt;- summary(MovieLense@data) %&gt;% as.data.frame()\n\nnames(films_rld) &lt;- c(\"user_id\", \"film_id\", \"rating\")\n\nhead(films_rld)\n\n  user_id film_id rating\n1       1       1      5\n2       2       1      4\n3       5       1      4\n4       6       1      4\n5      10       1      4\n6      13       1      3\n\n\nThe MovieLenseMeta dataset shows the genre classification for each film, in the same order as the dimension names in MovieLense:\n\nMovieLenseMeta %&gt;% \n  head(1) %&gt;% \n  mutate(across(everything(), as.character)) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  mutate(value = substring(value, 1, 40))\n\n# A tibble: 22 × 2\n   name       value                                   \n   &lt;chr&gt;      &lt;chr&gt;                                   \n 1 title      Toy Story (1995)                        \n 2 year       1995                                    \n 3 url        http://us.imdb.com/M/title-exact?Toy%20S\n 4 unknown    0                                       \n 5 Action     0                                       \n 6 Adventure  0                                       \n 7 Animation  1                                       \n 8 Children's 1                                       \n 9 Comedy     1                                       \n10 Crime      0                                       \n# ℹ 12 more rows\n\n\nAdd the genres to each film (the same film may have more than one genre):\n\nfilm_genres &lt;- MovieLenseMeta %&gt;% \n  mutate(film_id = row_number()) %&gt;% \n  clean_names() %&gt;% \n  select(-year ,-unknown, -url) \n\nhead(film_genres, 5)\n\n              title action adventure animation childrens comedy crime\n1  Toy Story (1995)      0         0         1         1      1     0\n2  GoldenEye (1995)      1         1         0         0      0     0\n3 Four Rooms (1995)      0         0         0         0      0     0\n4 Get Shorty (1995)      1         0         0         0      1     0\n5    Copycat (1995)      0         0         0         0      0     1\n  documentary drama fantasy film_noir horror musical mystery romance sci_fi\n1           0     0       0         0      0       0       0       0      0\n2           0     0       0         0      0       0       0       0      0\n3           0     0       0         0      0       0       0       0      0\n4           0     1       0         0      0       0       0       0      0\n5           0     1       0         0      0       0       0       0      0\n  thriller war western film_id\n1        0   0       0       1\n2        1   0       0       2\n3        1   0       0       3\n4        0   0       0       4\n5        1   0       0       5\n\n\nBuild the genre dataset by joining the film respondent level dataset with the film genres:\n\ngenres_rld &lt;- films_rld %&gt;% \n  inner_join(film_genres, by = \"film_id\") %&gt;% \n  pivot_longer(action:western, names_to = \"genre\") %&gt;% \n  filter(value == 1) %&gt;% \n  group_by(user_id, genre) %&gt;% \n  summarise(n = n()) %&gt;% \n  ungroup()\n\nhead(genres_rld)\n\n# A tibble: 6 × 3\n  user_id genre         n\n    &lt;int&gt; &lt;chr&gt;     &lt;int&gt;\n1       1 action       75\n2       1 adventure    42\n3       1 animation    12\n4       1 childrens    25\n5       1 comedy       91\n6       1 crime        25\n\n\n\nall_genres &lt;- levels(factor(genres_rld$genre))\n\ngenres_rld %&gt;% \n  group_by(genre) %&gt;% \n  summarise(\n    reach = n_distinct(user_id),\n    n = sum(n)\n  ) %&gt;% \n  ggplot(aes(x = reach, y = genre)) +\n  geom_bar(stat = \"identity\", fill = \"slateblue\") +\n  scale_y_discrete(limits=rev) +\n  labs(title = \"Movielens: reach by genre\")\n\n\n\n\n\n\n\n\nThen for each genre what is the distribution of number of viewed items?\n\ngenres_rld_wide &lt;- genres_rld %&gt;% \n  # pivot wider so we can add the zero values:\n  pivot_wider(names_from = genre, values_from = n, values_fill = 0) %&gt;% \n  rename(id = user_id)\n\nhead(genres_rld_wide)\n\n# A tibble: 6 × 19\n     id action adventure animation childrens comedy crime documentary drama\n  &lt;int&gt;  &lt;int&gt;     &lt;int&gt;     &lt;int&gt;     &lt;int&gt;  &lt;int&gt; &lt;int&gt;       &lt;int&gt; &lt;int&gt;\n1     1     75        42        12        25     91    25           5   106\n2     2     10         3         1         4     16     9           0    34\n3     3     14         4         0         0     12     9           1    19\n4     4      8         4         0         0      4     4           1     5\n5     5     56        33        14        29     82     9           0    27\n6     6     25        21        10        19     66    14           1   102\n# ℹ 10 more variables: fantasy &lt;int&gt;, film_noir &lt;int&gt;, horror &lt;int&gt;,\n#   musical &lt;int&gt;, mystery &lt;int&gt;, romance &lt;int&gt;, sci_fi &lt;int&gt;, thriller &lt;int&gt;,\n#   war &lt;int&gt;, western &lt;int&gt;\n\n\nKeep a record of the distribution of the volume metric for each genre:\n\nactual_distr &lt;- genres_rld_wide %&gt;% \n  pivot_longer(all_of(all_genres)) %&gt;% \n  group_by(name, value) %&gt;% \n  summarise(n = n()) %&gt;% \n  group_by(name) %&gt;% \n  mutate(\n    pc = n / sum(n),\n    cumpc = cumsum(pc)\n    ) %&gt;% \n  ungroup()\n\nhead(actual_distr)\n\n# A tibble: 6 × 5\n  name   value     n      pc   cumpc\n  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 action     0     5 0.00530 0.00530\n2 action     1    11 0.0117  0.0170 \n3 action     2    20 0.0212  0.0382 \n4 action     3    32 0.0339  0.0721 \n5 action     4    30 0.0318  0.104  \n6 action     5    44 0.0467  0.151  \n\n\nPlot the distribution of the margins:\n\nactual_distr %&gt;% \n  filter(value &lt;= 10) %&gt;% \n  ggplot(aes(x = value, y = pc, fill = name)) +\n  geom_bar(stat = \"identity\") +\n  scale_x_continuous(breaks = 0:10) +\n  facet_wrap(~name, scales = \"free\", ncol = 3) +\n  theme(legend.position = \"none\") +\n  labs(title = \"MovieLense: distribution of number of films watched (truncated at 10)\")\n\n\n\n\n\n\n\n\nThis dataset shows a variety of discrete distributions representing the number of films each person watched for each genre. Note that some distributions like Drama and Romance don’t have a zero value. Everyone in this dataset has watched at least one film.\n\n\n3.1.2 simulation using Gaussian copula\nThe Gaussian copula is the easiest to implement as it doesn’t require installing any additional package in R. It can be created using MASS::mvrnorm.\nTo create a simulation of the genres_rld dataset, we first calculate its Spearman correlation:\n\nspearman_cor &lt;- genres_rld_wide %&gt;% \n  select(-id) %&gt;% \n  cor(method = \"spearman\")\n\nspearman_cor[1:5, 1:5]\n\n             action adventure animation childrens    comedy\naction    1.0000000 0.9461663 0.7033189 0.7288907 0.7867559\nadventure 0.9461663 1.0000000 0.7735571 0.7928897 0.8215120\nanimation 0.7033189 0.7735571 1.0000000 0.8716790 0.7602097\nchildrens 0.7288907 0.7928897 0.8716790 1.0000000 0.8066026\ncomedy    0.7867559 0.8215120 0.7602097 0.8066026 1.0000000\n\n\nNext, we generate a random sample of 100k units from a multivariate Normal using these observed correlations:\n\nn &lt;- 100000\n\nset.seed(13)\n\nsimdata &lt;- MASS::mvrnorm(\n  n = n, \n  mu = rep(0, nrow(spearman_cor)), \n  Sigma = spearman_cor\n  )\n\nsummary(simdata[1:5,])\n\n     action          adventure         animation           childrens      \n Min.   :-1.3817   Min.   :-1.6258   Min.   :-2.351214   Min.   :-2.3494  \n 1st Qu.:-0.9830   1st Qu.:-1.1368   1st Qu.:-1.360031   1st Qu.:-0.8189  \n Median :-0.4934   Median :-0.7353   Median :-0.682401   Median :-0.7727  \n Mean   :-0.5722   Mean   :-0.7217   Mean   :-0.751267   Mean   :-0.6851  \n 3rd Qu.:-0.3034   3rd Qu.:-0.3517   3rd Qu.:-0.008015   3rd Qu.: 0.2231  \n Max.   : 0.3006   Max.   : 0.2410   Max.   : 0.645323   Max.   : 0.2925  \n     comedy            crime          documentary           drama        \n Min.   :-1.3483   Min.   :-1.9235   Min.   :-2.86456   Min.   :-1.8829  \n 1st Qu.:-0.9705   1st Qu.:-0.6761   1st Qu.:-0.69672   1st Qu.:-0.9353  \n Median :-0.7107   Median : 0.1265   Median :-0.03413   Median :-0.7946  \n Mean   :-0.5316   Mean   :-0.3785   Mean   :-0.47859   Mean   :-0.7699  \n 3rd Qu.:-0.4503   3rd Qu.: 0.2301   3rd Qu.: 0.25707   3rd Qu.:-0.3934  \n Max.   : 0.8218   Max.   : 0.3506   Max.   : 0.94537   Max.   : 0.1568  \n    fantasy          film_noir           horror           musical       \n Min.   :-1.1568   Min.   :-1.5314   Min.   :-1.4874   Min.   :-1.8693  \n 1st Qu.:-0.8651   1st Qu.:-0.6781   1st Qu.:-0.7694   1st Qu.:-1.0213  \n Median :-0.2646   Median :-0.4907   Median :-0.5717   Median :-1.0016  \n Mean   :-0.1923   Mean   :-0.5074   Mean   :-0.5355   Mean   :-0.7014  \n 3rd Qu.: 0.4770   3rd Qu.:-0.1773   3rd Qu.:-0.2256   3rd Qu.:-0.2046  \n Max.   : 0.8482   Max.   : 0.3405   Max.   : 0.3767   Max.   : 0.5898  \n    mystery           romance            sci_fi           thriller      \n Min.   :-1.5326   Min.   :-1.6708   Min.   :-0.9721   Min.   :-1.5443  \n 1st Qu.:-0.3824   1st Qu.:-1.1107   1st Qu.:-0.7636   1st Qu.:-0.8727  \n Median :-0.1736   Median :-0.6660   Median :-0.4596   Median :-0.5293  \n Mean   :-0.2116   Mean   :-0.7377   Mean   :-0.3985   Mean   :-0.5975  \n 3rd Qu.: 0.2609   3rd Qu.:-0.5148   3rd Qu.:-0.3145   3rd Qu.:-0.2432  \n Max.   : 0.7697   Max.   : 0.2736   Max.   : 0.5173   Max.   : 0.2019  \n      war              western       \n Min.   :-1.28294   Min.   :-2.0486  \n 1st Qu.:-0.99671   1st Qu.:-0.9132  \n Median :-0.48773   Median :-0.8956  \n Mean   :-0.49834   Mean   :-0.7299  \n 3rd Qu.: 0.00547   3rd Qu.:-0.4066  \n Max.   : 0.27018   Max.   : 0.6147  \n\n\nWe now have a multivariate normal where the margins are \\(N(0,1)\\) and the correlation is the observed Spearman correlation of the original dataset.\nNext we map the cumulative distributions of these \\(N(0,1)\\) margins to the observed cumulative distributions.\nLet’s walk through the process using the Documentary genre as it has the lowest number of distinct values:\n\ni = 7 # index for Documentaries\n\nactual_var_distr &lt;- actual_distr %&gt;%\n    filter(name == all_genres[i])\n\nactual_var_distr %&gt;% \n  summarise(\n    n = n(),\n    min = min(value),\n    max = max(value)\n  )\n\n# A tibble: 1 × 3\n      n   min   max\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1    13     0    20\n\n\nWe have 13 distinct possible values for this distribution, ranging from 0 to 20.\n\nactual_var_distr %&gt;% \n  ungroup() %&gt;% \n  mutate(prev_cumpc = ifelse(is.na(lag(cumpc)), 0, lag(cumpc))) %&gt;% \n  filter(value &lt; 3 | value &gt; 10) %&gt;% \n  select(value, n,  prev_cumpc, cumpc)\n\n# A tibble: 6 × 4\n  value     n prev_cumpc cumpc\n  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1     0   591      0     0.627\n2     1   183      0.627 0.821\n3     2    83      0.821 0.909\n4    13     2      0.996 0.998\n5    16     1      0.998 0.999\n6    20     1      0.999 1    \n\n\nFollowing the actual cumulative distribution, in our simulated datset we will also want:\n- anything under 62.7% to be mapped to 0.\n- anything between 62.7% and 82.1% to be mapped to 1.\n- etc….\n- anything &gt; 99.9% to be mapped to 20.\nTake the corresponding margin in our simulation:\n\ns &lt;- simdata[, i]\n\nsummary(s)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-4.342483 -0.668564  0.002788  0.003698  0.676800  4.573686 \n\n\nPlot the cumulative distribution of our actual margin and the result of the MVN simulation before transformation:\n\nrbind(\n  # actual cumulative distribution for genre = \"action\":\n  actual_var_distr %&gt;% \n    mutate(source = \"actual\") %&gt;% \n    ungroup() %&gt;% \n    select(source, value, cumpc),\n  \n  # untransformed simulated cumulative distribution for genre = \"action\":\n  data.frame(\n    name = all_genres[i],\n    value = round(s, digits = 1)\n    ) %&gt;% \n    group_by(name, value) %&gt;% \n    summarise(n = n()) %&gt;% \n    mutate(\n      pc = n / sum(n),\n      cumpc = cumsum(pc),\n      source = \"sim\"\n      ) %&gt;% \n    ungroup() %&gt;% \n    select(source, value, cumpc)\n) %&gt;% \n  ggplot(aes(x = value, y = cumpc, col = source)) +\n  geom_line() +\n  scale_color_manual(values=c(\"slateblue1\", \"darkorange\")) + \n  facet_wrap(~source, scales = \"free\") +\n  theme(legend.position = \"none\") +\n  labs(title = \"cumulative distribution for genre = documentary\")\n\n\n\n\n\n\n\n\nThe untransformed simulated margin in orange is the cumulative distribution of a \\(N(0,1)\\) and we want to transform all its values so it looks like the observed margin in blue.\nFind the critical values so we can map the simulated values to these probabilities:\n\nquantile(s, actual_var_distr$cumpc)\n\n62.67232% 82.07847% 90.88017% 94.27359% 96.92471% 98.40933% 98.72747%  99.0456% \n0.3258383 0.9196472 1.3338876 1.5817369 1.8715100 2.1545824 2.2452217 2.3575869 \n99.46978% 99.57582% 99.78791% 99.89396%      100% \n2.5733421 2.6331406 2.8528658 3.0762966 4.5736860 \n\n\nSo in our simulated margin, we will map:\n- anything that is \\(\\leq 0.3258\\) to zero,\n- anything that is \\(&gt; 0.3258\\) and \\(\\leq 0.9196\\) to one,\n- etc…\n- anything that is \\(&gt; 4.574\\) to 20\nTo define the breaks for the cut function, we also need a lower bound which is the minimum of the simulated margin:\n\ncrv = as.vector(c(min(s), quantile(s, actual_var_distr$cumpc)))\ncrv\n\n [1] -4.3424830  0.3258383  0.9196472  1.3338876  1.5817369  1.8715100\n [7]  2.1545824  2.2452217  2.3575869  2.5733421  2.6331406  2.8528658\n[13]  3.0762966  4.5736860\n\n\nShow what happens when we apply these breaks to our simulated margin:\n\ndata.frame(s = s) %&gt;% \n  mutate(cuts = cut(\n    s, \n    breaks = crv,\n    right = TRUE,\n    include.lowest = TRUE\n    )\n    ) %&gt;% \n  group_by(cuts) %&gt;% \n  summarise(min = min(s), max = max(s), n = n())\n\n# A tibble: 14 × 4\n   cuts             min   max     n\n   &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1 [-4.34,0.326] -4.34  0.326 62672\n 2 (0.326,0.92]   0.326 0.920 19406\n 3 (0.92,1.33]    0.920 1.33   8802\n 4 (1.33,1.58]    1.33  1.58   3393\n 5 (1.58,1.87]    1.58  1.87   2651\n 6 (1.87,2.15]    1.87  2.15   1485\n 7 (2.15,2.25]    2.15  2.25    318\n 8 (2.25,2.36]    2.25  2.36    318\n 9 (2.36,2.57]    2.36  2.57    424\n10 (2.57,2.63]    2.57  2.63    106\n11 (2.63,2.85]    2.63  2.85    212\n12 (2.85,3.08]    2.85  3.07    106\n13 (3.08,4.57]    3.08  4.38    106\n14 &lt;NA&gt;           4.57  4.57      1\n\n\nNote that the last cut isn’t quite right: due to rounding, the maximum value of the distribution ends up in the NA category, when it should be in the last break. This will have to be corrected when we apply the critical values.\nCut the simulated normal data according to the breaks given by the crv table:\n\ntemp &lt;- cut(\n    s,\n    right = TRUE,\n    include.lowest = TRUE,\n    breaks = crv,\n    labels = actual_var_distr$value\n  )\n\nsummary(temp)\n\n    0     1     2     3     4     5     6     7     8    10    13    16    20 \n62672 19406  8802  3393  2651  1485   318   318   424   106   212   106   106 \n NA's \n    1 \n\n\nNote that we have named levels (factors) which will have to be converted to numeric. Also, the NA entry should be converted to the maximum value.\n\nhead(temp)\n\n[1] 2 0 0 0 0 1\nLevels: 0 1 2 3 4 5 6 7 8 10 13 16 20\n\n\n\ns[is.na(temp)]\n\n[1] 4.573686\n\n\nConvert the factor levels back to numeric values:\n\nsim_temp &lt;-data.frame(value = as.numeric(as.character(temp)))\n\nsummary(sim_temp)\n\n     value        \n Min.   : 0.0000  \n 1st Qu.: 0.0000  \n Median : 0.0000  \n Mean   : 0.8038  \n 3rd Qu.: 1.0000  \n Max.   :20.0000  \n NA's   :1        \n\n\nIf by bad luck we have an NA, this is due to the rounding of the crv, so we replace NA with the max value:\n\nsim_temp[is.na(sim_temp)] &lt;- max(actual_var_distr$value, na.rm = TRUE)\n\nsummary(sim_temp)\n\n     value       \n Min.   : 0.000  \n 1st Qu.: 0.000  \n Median : 0.000  \n Mean   : 0.804  \n 3rd Qu.: 1.000  \n Max.   :20.000  \n\n\nOnce transformed in this way, our simulated margin matches the observed distribution exactly:\n\nrbind(\n  # actual cumulative distribution for genre = \"documentary\":\n  actual_var_distr %&gt;% \n    mutate(source = \"actual\") %&gt;% \n    ungroup() %&gt;% \n    select(source, value, cumpc),\n  \n  # untransformed simulated cumulative distribution for genre = \"documentary\":\n  data.frame(\n    genre = all_genres[i],\n    value = round(s, digits = 1)\n    ) %&gt;% \n    group_by(genre, value) %&gt;% \n    summarise(n = n()) %&gt;% \n    mutate(\n      pc = n / sum(n),\n      cumpc = cumsum(pc),\n      source = \"sim untransformed\"\n      ) %&gt;% \n    ungroup() %&gt;% \n    select(source, value, cumpc),\n  \n  # transformed simulated cumulative distribution for genre = \"documentary\":\n  data.frame(\n    value = sim_temp\n    ) %&gt;% \n    group_by(value) %&gt;% \n    summarise(n = n()) %&gt;% \n    mutate(\n      pc = n / sum(n),\n      cumpc = cumsum(pc),\n      source = \"sim transformed\"\n      ) %&gt;% \n    ungroup() %&gt;% \n    select(source, value, cumpc)\n) %&gt;% \n  mutate(\n    source = factor(\n    source, \n    levels = c(\"actual\", \"sim untransformed\", \"sim transformed\"))\n    ) %&gt;% \n  ggplot(aes(x = value, y = cumpc, col = source)) +\n  geom_line() +\n  scale_color_manual(values=c(\"slateblue1\", \"violetred\", \"darkorange\")) + \n  facet_wrap(~source, scales = \"free\") +\n  theme(legend.position = \"none\") +\n  labs(title = \"cumulative distribution for genre = documentary\")\n\n\n\n\n\n\n\n\nThe resulting transformed margin in orange matches the actual margin in blue exactly.\nLet’s apply this transformation to each margin:\n\nsimulation_fc &lt;- function(actual_distr, cor_mat, sample_size){\n  \n  #' Calculates the items joint reach per weight of usage category.\n  #'\n  #' @param actual_distr A data frame containing the name, value, n, pc, cumpc for each variable.\n  #' @param cor_mat A matrix containing the correlation.\n  #' @param sample_size The number of units required for the simulated dataset\n  #' @return The simulated dataset. \n  #' \n  simdata &lt;- MASS::mvrnorm(\n    n = sample_size, \n    mu = rep(0, nrow(cor_mat)), \n    Sigma = cor_mat\n    )\n  \n  # make copy so we can replace each column with their corrected values:\n  sim_copy &lt;- simdata\n  \n  names &lt;- colnames(cor_mat)\n\n  for (i in 1:length(names)) {\n  \n  s &lt;- sim_copy[, i]\n  \n  actual_var_distr &lt;- actual_distr %&gt;%\n    filter(name == names[i])\n  \n  # Find the critical values so we can map the simulated values to these probabilities:\n  crv = as.vector(c(min(s), quantile(s, actual_var_distr$cumpc)))\n  \n  # Cut the simulated normal data according to the breaks given by the crv table:\n  temp &lt;- cut(\n    s,\n    right = TRUE,\n    include.lowest = TRUE,\n    breaks = crv,\n    labels = actual_var_distr$value\n  )\n  \n  # Convert the levels back to numeric values:\n  sim_temp &lt;-\n    data.frame(value = as.numeric(as.character(temp)))\n  \n  # if by bad luck we have an NA, this may be to do with rounding of the crv...\n  # =&gt; replace with the max value:\n  sim_temp[is.na(sim_temp)] &lt;- max(actual_var_distr$value, na.rm = TRUE)\n  \n  # transform these levels back into our values:\n  sim_copy[, i] &lt;- sim_temp$value\n  \n  }\n  \n  return(as.data.frame(sim_copy))\n  \n}\n\n\nsim_db &lt;- simulation_fc(\n    actual_distr = actual_distr,\n    cor_mat = spearman_cor,\n    sample_size = 100000\n)\n\nhead(sim_db)\n\n  action adventure animation childrens comedy crime documentary drama fantasy\n1     21        10         1         1     26    12           0    37       0\n2      4         2         1         1      6     2           1    14       0\n3     16         5         1         4     24     4           0    36       1\n4     25        19         2        10     27     4           0    34      12\n5     22        16         1         2     19     3           2    19       1\n6     38        21         3         4     23     9           0    57       0\n  film_noir horror musical mystery romance sci_fi thriller war western\n1         2      1       1      11      32     10       23   9       0\n2         1      0       1       3       9      2        4   3       0\n3         0      4       3       6      20      4       22  12       3\n4         1      8       8       4      29     11       14   7       2\n5         0      4       0       3       8     14       25   4       0\n6         2      4       4       9      55     17       39  17       2\n\n\nCompare simulated mean against actual mean by genre:\n\ngenres_rld_wide %&gt;%\npivot_longer(all_of(all_genres)) %&gt;%\ngroup_by(name) %&gt;%\nsummarise(actual = sum(value)/nrow(genres_rld_wide)) %&gt;%\nleft_join(\n  sim_db %&gt;%\n    pivot_longer(all_of(all_genres)) %&gt;%\n    group_by(name) %&gt;%\n    summarise(sim = sum(value)/nrow(sim_db)),\n    by = \"name\"\n) %&gt;%\nmutate(diff = round(actual - sim, digits = 3))\n\n# A tibble: 18 × 4\n   name        actual    sim   diff\n   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 action      27.1   27.1   -0.001\n 2 adventure   14.5   14.5    0    \n 3 animation    3.82   3.82   0    \n 4 childrens    7.57   7.58   0    \n 5 comedy      31.6   31.6   -0.001\n 6 crime        8.51   8.51   0    \n 7 documentary  0.804  0.804  0    \n 8 drama       41.8   41.8   -0.002\n 9 fantasy      1.43   1.43   0    \n10 film_noir    1.84   1.84   0    \n11 horror       5.60   5.60   0    \n12 musical      5.25   5.25   0    \n13 mystery      5.55   5.55   0    \n14 romance     20.4   20.4    0    \n15 sci_fi      13.5   13.5    0    \n16 thriller    23.1   23.1   -0.001\n17 war          9.97   9.97   0    \n18 western      1.97   1.97   0    \n\n\nThe simulated dataset preserves the average number of films per person for all genres.\n\n\n3.1.3 univariate comparison\nAs we have mapped each \\(N(0,1)\\) margin to the observed marginal distributions, there should be no difference between actual and simulated margins:\n\nrbind(\n  \n  actual_distr %&gt;% \n    select(-cumpc) %&gt;% \n    mutate(source = \"actual\"),\n  \n  sim_db %&gt;% \n    pivot_longer(all_of(all_genres)) %&gt;% \n    group_by(name, value) %&gt;% \n    summarise(n = n()) %&gt;% \n    group_by(name) %&gt;% \n    mutate(pc = n / sum(n)) %&gt;% \n    mutate(source = \"sim\")\n  \n) %&gt;% \n  ggplot(aes(x = value, y = pc, col = source)) +\n  geom_line() +\n  facet_wrap(~name, scales = \"free\", ncol = 3) +\n  labs(title = \"actual vs simulated genre distributions\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThe margins are an exact match (the actual and simulated densities are the same and can’t be distinguished in the above plots) so any subsequent calculations based on summing the values will be preserved.\n\n\n3.1.4 bivariate comparison\nTo start with, we can compare the Spearman correlations in the original and simulated datasets:\n\ncompare_corrs &lt;- rbind(\n  \n  # actual correlations:\n  spearman_cor %&gt;% \n    as.data.frame() %&gt;% \n    rownames_to_column(\"genre1\") %&gt;% \n    pivot_longer(all_of(all_genres), names_to = \"genre2\", values_to = \"cor\") %&gt;% \n    filter(genre1 &gt; genre2) %&gt;% \n    mutate(source = \"actual\"),\n  \n  # simulated transformed correlations:\n  cor(sim_db, method = \"spearman\") %&gt;% \n    as.data.frame() %&gt;% \n    rownames_to_column(\"genre1\") %&gt;% \n    pivot_longer(all_of(all_genres), names_to = \"genre2\", values_to = \"cor\") %&gt;% \n    filter(genre1 &gt; genre2) %&gt;% \n    mutate(source = \"sim\")\n) %&gt;% \n  pivot_wider(names_from = \"source\", values_from = \"cor\")\n\nhead(compare_corrs)\n\n# A tibble: 6 × 4\n  genre1    genre2    actual   sim\n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 adventure action     0.946 0.939\n2 animation action     0.703 0.661\n3 animation adventure  0.774 0.733\n4 childrens action     0.729 0.704\n5 childrens adventure  0.793 0.770\n6 childrens animation  0.872 0.837\n\n\n\ncompare_corrs %&gt;% \n  ggplot(aes(x = actual, y = sim)) +\n  geom_point(col = \"maroon\") +\n  geom_abline(slope = 1,  col = \"grey\") +\n  labs(\n    title = \"genre pairwise Spearman correlations\",\n    x = \"actual correlations\", y = \"simulated correlations\"\n    )\n\n\n\n\n\n\n\n\nWe see that the pairwise correlations are always a little bit lower than their original correlations but they’re still very close.\nThe correlation is just one summary value for the entire bivariate distribution, but how well did this work in the entire distribution? Do we have the expected percentages in the tails of the bivariate distributions? To illustrate this we create 4 categories for each univariate:\n- cat0: non-users.\n- cat1: light users (first tertile based on non-null values).\n- cat2: medium users (second tertile based on non-null values).\n- cat3: heavy users (third tertile based on non-null values).\nThen we calculate the percentage reach for all pairs of genres and all categories: P(X = cat0 and Y = cat0) etc…\n\n# create all possible pairs of genres:\nmy_grid &lt;- expand.grid(\n  var1 = all_genres,\n  var2 = all_genres,\n  stringsAsFactors = FALSE\n) %&gt;% \n  arrange(var1, var2) %&gt;% \n  filter(var1 &lt; var2)\n\njoint_reach_categories_fc &lt;- function(my_grid, actual_db, sim_db, my_source){\n  \n  #' Calculates the items joint reach per weight of usage category.\n  #'\n  #' @param my_grid A data frame containing all distinct pairwise combinations.\n  #' @param actual_db A data frame containing an id column and one column per item.\n  #' @param sim_db A data frame containing one column per item.\n  #' @param my_source A string containing the name of the source.\n  #' @return The pairwise reach for each item combination and each category. \n  #' \n   \n  my_list &lt;- list()\n\n  for(i in 1:nrow(my_grid)){\n  \n    var1 &lt;- my_grid$var1[i]\n    var2 &lt;- my_grid$var2[i]\n  \n    actual_long &lt;- rbind(\n    \n    # separate the zeros and assign tertile = 0:\n    actual_db %&gt;% \n      select(all_of(c(\"id\", var1, var2))) %&gt;% \n      pivot_longer(var1:var2, names_to = \"var\", values_to = \"n\") %&gt;% \n      filter(\n        var %in% c(var1, var2),\n        n == 0\n        ) %&gt;% \n      group_by(var) %&gt;% \n      mutate(tertile = 0),\n      \n    # calculate the tertile of each distribution excluding zero:\n    actual_db %&gt;% \n      select(all_of(c(\"id\", var1, var2))) %&gt;% \n      pivot_longer(var1:var2, names_to = \"var\", values_to = \"n\") %&gt;% \n      filter(\n        var %in% c(var1, var2),\n        n &gt; 0\n        ) %&gt;% \n      group_by(var) %&gt;% \n      mutate(tertile = ntile(n, 3))\n  )\n\n    actual_wide &lt;- actual_long %&gt;% \n      select(-n) %&gt;% \n      pivot_wider(names_from = var, values_from = tertile) \n    \n    # same with the results from the simulation:\n    \n  sim_long &lt;- rbind(\n    \n    # separate the zeros and assign tertile = 0:\n    sim_db %&gt;% \n      mutate(id = row_number()) %&gt;% \n      select(all_of(c(\"id\", var1, var2))) %&gt;% \n      pivot_longer(var1:var2, names_to = \"var\", values_to = \"n\") %&gt;% \n      filter(\n        var %in% c(var1, var2),\n        n == 0\n        ) %&gt;% \n      group_by(var) %&gt;% \n      mutate(tertile = 0),\n      \n    # calculate the tertile of each distribution excluding zero:\n    sim_db %&gt;% \n      mutate(id = row_number()) %&gt;% \n      select(all_of(c(\"id\", var1, var2))) %&gt;% \n      pivot_longer(var1:var2, names_to = \"var\", values_to = \"n\") %&gt;% \n      filter(\n        var %in% c(var1, var2),\n        n &gt; 0\n        ) %&gt;% \n      group_by(var) %&gt;% \n      mutate(tertile = ntile(n, 3))\n  )\n\n    sim_wide &lt;- sim_long %&gt;% \n      select(-n) %&gt;% \n      pivot_wider(names_from = var, values_from = tertile) \n    \n    # add results to the list:\n   my_list[[i]] &lt;- rbind(\n     \n     actual_wide %&gt;% \n      summarise(\n        cat0 = sum(!!sym(var1) == 0 & !!sym(var2) == 0)/nrow(actual_wide),\n        cat1 = sum(!!sym(var1) == 1 & !!sym(var2) == 1)/nrow(actual_wide),\n        cat2 = sum(!!sym(var1) == 2 & !!sym(var2) == 2)/nrow(actual_wide),\n        cat3 = sum(!!sym(var1) == 3 & !!sym(var2) == 3)/nrow(actual_wide)\n      ) %&gt;% \n     mutate(source = \"actual\", var1 = var1, var2 = var2),\n     \n     sim_wide %&gt;% \n      summarise(\n        cat0 = sum(!!sym(var1) == 0 & !!sym(var2) == 0)/nrow(sim_wide),\n        cat1 = sum(!!sym(var1) == 1 & !!sym(var2) == 1)/nrow(sim_wide),\n        cat2 = sum(!!sym(var1) == 2 & !!sym(var2) == 2)/nrow(sim_wide),\n        cat3 = sum(!!sym(var1) == 3 & !!sym(var2) == 3)/nrow(sim_wide)\n      ) %&gt;% \n     mutate(source = my_source, var1 = var1, var2 = var2)\n   )\n\n  }\n  \n  results &lt;- bind_rows(my_list)\n  \n  return(results)\n  \n}\n\n\nresults &lt;- joint_reach_categories_fc(\n  my_grid = my_grid, \n  actual_db = genres_rld_wide, \n  sim_db = sim_db,\n  my_source = \"sim\"\n    )\n\nhead(results)\n\n# A tibble: 6 × 7\n     cat0   cat1   cat2  cat3 source var1   var2     \n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;    \n1 0.00318 0.266  0.259  0.298 actual action adventure\n2 0.00531 0.255  0.228  0.277 sim    action adventure\n3 0.00424 0.0976 0.125  0.199 actual action animation\n4 0.00522 0.0809 0.0980 0.170 sim    action animation\n5 0.00212 0.165  0.154  0.232 actual action childrens\n6 0.00492 0.140  0.127  0.201 sim    action childrens\n\n\n\ncats &lt;- paste0(\"cat\", 0:3)\nmy_cols &lt;- c(\"slateblue\", \"cornflowerblue\", \"royalblue1\",\"thistle\")\n\nfor(i in 1:length(cats)){\n  \n  p &lt;- results %&gt;% \n    select(all_of(c(\"source\", \"var1\", \"var2\", cats[i]))) %&gt;% \n    rename(cat = cats[i]) %&gt;% \n    pivot_wider(names_from = source, values_from = cat) %&gt;% \n    ggplot(aes(x = actual, y = sim)) +\n    geom_point(col = my_cols[i]) +\n    geom_abline(slope = 1,  col = \"grey\") +\n    labs(title = paste0(cats[i], \" : pairwise reach\"))\n  \n  plot(p)\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen looking at the pairwise distributions we notice that:\n- the non-users of both genres (cat0) tend to be slightly over-estimated but not much.\n- the heavy-users of both genres (cat3) are systematically under-estimated.\nMaybe the Gaussian copula isn’t the best choice for the dependence structure and we could do better with a copula that has a different shape at the tails. Overall this is still very close to the actual bivariate distributions.\n\n\n3.1.5 multivariate comparison\nIn the previous section we checked the combined reach levels for each pair of genres, but what happens when we calculate the combined reach for 2, 3, 4… all genres? As all users in this dataset have watch at least one drama and one romance film the fully combined reach across all genres will be 1, so let’s start with the smallest genres and add the next bigger genre at each step.\n\ngenres_ranking &lt;- genres_rld %&gt;% \n   group_by(genre) %&gt;% \n   summarise(reach = n()) %&gt;% \n   arrange(reach)\n\ngenres_ranking %&gt;% head()\n\n# A tibble: 6 × 2\n  genre       reach\n  &lt;chr&gt;       &lt;int&gt;\n1 documentary   352\n2 western       491\n3 fantasy       512\n4 film_noir     618\n5 animation     659\n6 musical       754\n\n\n\nmy_list &lt;- list()\n\nfor(i in 2:nrow(genres_ranking)){\n  \n  my_genres_combination &lt;- genres_ranking %&gt;% head(i)\n  \n  actual_mat &lt;- genres_rld_wide[, my_genres_combination$genre]\n  actual_comb_sum &lt;-  rowSums(actual_mat)\n  \n  sim_mat &lt;- sim_db[, my_genres_combination$genre]\n  sim_comb_sum &lt;-  rowSums(sim_mat)\n  \n  my_list[[i-1]] &lt;- data.frame(\n    genre_combination = paste(my_genres_combination$genre, collapse = \", \"),\n    n1 = ncol(actual_mat),\n    n2 = ncol(sim_mat),\n    actual_pc = sum(actual_comb_sum &gt; 0) / length(actual_comb_sum),\n    sim_pc = sum(sim_comb_sum &gt; 0) / length(sim_comb_sum)\n  )\n}\n\nmultivariate_comparison &lt;- bind_rows(my_list) \n\nmultivariate_comparison %&gt;% select(-n1, -n2) %&gt;% head(4)\n\n                                    genre_combination actual_pc  sim_pc\n1                                documentary, western 0.6277837 0.63941\n2                       documentary, western, fantasy 0.7391304 0.74815\n3            documentary, western, fantasy, film_noir 0.8600212 0.84687\n4 documentary, western, fantasy, film_noir, animation 0.9353128 0.89748\n\n\n\nmultivariate_comparison %&gt;% \n  ggplot(aes(x = actual_pc,  y = sim_pc)) +\n  geom_point(col = \"slateblue\") +\n  geom_abline(slope = 1,  col = \"grey\") +\n  scale_x_continuous(labels = scales::percent) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"V1: actual vs simulated combined reach\")\n\n\n\n\n\n\n\n\nFor the first couple of combinations the simulated result are slightly bigger that the actual combined reach but after that, we go in the other direction and the simulated result is slightly lower than the actual result. Note that as everyone in this dataset has watched at least one film, we get to 100% reach when looking across all genres.\n\n\n3.1.6 conclusion\nIn this section we demonstrated how we can recreate a simulation of a multivariate discrete distribution with a Gaussian copula using just their Spearman correlation and their empirical cumulative marginal distributions. The resulting simulation has margins that match the original ones exactly, so there is no loss of volume (in this case the total number of films watched), and the multivariate relationships have also been very well preserved."
  },
  {
    "objectID": "posts/cross_media_with_copulas/index.html#example-using-radio-listening-dataset",
    "href": "posts/cross_media_with_copulas/index.html#example-using-radio-listening-dataset",
    "title": "Estimating Cross Media Usage Using Copulas",
    "section": "3.2 example using radio listening dataset",
    "text": "3.2 example using radio listening dataset\nWhen working with media usage, distributions typically exhibit a high level of zero for the non-users. Here we show what happens when using the Spearman correlation and we compare it with the results using the Tetrachoric correlation.\nIn media planning, the negative binomial distribution (NBD) is widely use for Reach and Frequency models, based on the work of Mr A.S.C. Ehrenberg (Ehrenberg 1959). When fitting the NBD, the method typically used is not the method of moments or of maximum likelihood, but is based on preserving the probability at zero \\(P(X)=0\\) and the total volume (e.g. total impressions, purchases, …) via the mean. In doing so, the currencies are preserved (no loss in volume) and so is the observed reach. It is with these same objectives in mind of preserving reach and volume that the following approach is also based.\nIn this example, the dataset was created using R’s synthpop package (Dibben 2016) and is based on the radio listening (time spent in minutes) of 11 stations from RAJAR data from 2019Q1. It contains 100k synthetic respondents and is available from github\n\ndb &lt;- read.csv(\"syn_listening_db.csv\")\nsummary(db)\n\n       id              st1               st2                st3        \n Min.   :     1   Min.   :   0.00   Min.   :   0.000   Min.   :   0.0  \n 1st Qu.: 25001   1st Qu.:   0.00   1st Qu.:   0.000   1st Qu.:   0.0  \n Median : 50000   Median :   0.00   Median :   0.000   Median :   0.0  \n Mean   : 50000   Mean   :  61.94   Mean   :   3.532   Mean   : 229.6  \n 3rd Qu.: 75000   3rd Qu.:   0.00   3rd Qu.:   0.000   3rd Qu.: 120.0  \n Max.   :100000   Max.   :7530.00   Max.   :5640.000   Max.   :7830.0  \n      st4               st5              st6               st7         \n Min.   :   0.00   Min.   :   0.0   Min.   :   0.00   Min.   :   0.00  \n 1st Qu.:   0.00   1st Qu.:   0.0   1st Qu.:   0.00   1st Qu.:   0.00  \n Median :   0.00   Median :   0.0   Median :   0.00   Median :   0.00  \n Mean   :  15.76   Mean   : 153.3   Mean   :  16.62   Mean   :  42.46  \n 3rd Qu.:   0.00   3rd Qu.:   0.0   3rd Qu.:   0.00   3rd Qu.:   0.00  \n Max.   :6495.00   Max.   :7695.0   Max.   :5640.00   Max.   :5895.00  \n      st8                st9               st10               st11         \n Min.   :   0.000   Min.   :   0.00   Min.   :   0.000   Min.   :   0.000  \n 1st Qu.:   0.000   1st Qu.:   0.00   1st Qu.:   0.000   1st Qu.:   0.000  \n Median :   0.000   Median :   0.00   Median :   0.000   Median :   0.000  \n Mean   :   4.179   Mean   :  26.68   Mean   :   1.768   Mean   :   8.761  \n 3rd Qu.:   0.000   3rd Qu.:   0.00   3rd Qu.:   0.000   3rd Qu.:   0.000  \n Max.   :1965.000   Max.   :6360.00   Max.   :4200.000   Max.   :5745.000  \n\n\nGiven just a correlation matrix and the empirical marginal cumulative distributions, can we recreate a simulated dataset that preserves the multivariate distribution of the original dataset?\n\nstations &lt;- names(db)[2:ncol(db)]\n  \ndb %&gt;% \n  pivot_longer(all_of(stations), names_to = \"station\") %&gt;% \n  mutate(station = factor(station, levels = stations)) %&gt;% \n  group_by(station) %&gt;% \n  summarise(reach_pc = sum(value &gt; 0) / n()) %&gt;% \n  ggplot(aes(x = reach_pc, y = station)) +\n  geom_bar(stat = \"identity\", fill = \"slateblue\") +\n  scale_x_continuous(labels = scales::percent) +\n  scale_y_discrete(limits=rev) +\n  labs(title = \"Radio: reach% by station\", x = \"reach%\")\n\n\n\n\n\n\n\n\nThen for each station what is the distribution of time spent in minutes?\n\nactual_distr &lt;- db %&gt;% \n  pivot_longer(all_of(stations)) %&gt;% \n  group_by(name, value) %&gt;% \n  summarise(n = n()) %&gt;% \n  group_by(name) %&gt;% \n  mutate(\n    pc = n / sum(n),\n    cumpc = cumsum(pc)\n    ) \n\nhead(actual_distr)\n\n# A tibble: 6 × 5\n# Groups:   name [1]\n  name  value     n      pc cumpc\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 st1       0 83461 0.835   0.835\n2 st1       5     9 0.00009 0.835\n3 st1       7    56 0.00056 0.835\n4 st1      10     9 0.00009 0.835\n5 st1      12     4 0.00004 0.835\n6 st1      15   916 0.00916 0.845\n\n\nPlot the distribution of the margins, excluding zeros:\n\nactual_distr %&gt;% \n  filter(value &gt; 0) %&gt;% \n  mutate(hours = cut(value/60, breaks = 0:135)) %&gt;% \n  group_by(name, hours) %&gt;% \n  summarise(n = sum(n)) %&gt;% \n  ggplot(aes(x = hours, y = n, fill = name)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~name, scales = \"free\", ncol = 3) +\n  theme(\n    legend.position = \"none\",\n    axis.text.x=element_blank(),\n    axis.ticks.x=element_blank()\n    ) +\n  labs(title = \"radio: distribution of number of hours (excl. zero)\")\n\n\n\n\n\n\n\n\nThe distribution of time spent is similar for all stations: it dies away quickly and has a long tail.\n\n3.2.1 using Spearman correlation\nTo create a simulation of the radio dataset, we first calculate its Spearman correlation:\n\nspearman_cor &lt;- db %&gt;% \n  select(-id) %&gt;% \n  cor(method = \"spearman\")\n\nspearman_cor[1:5, 1:5]\n\n            st1          st2          st3          st4         st5\nst1  1.00000000  0.102310851 -0.038459515 -0.045758896 -0.11805906\nst2  0.10231085  1.000000000 -0.042944399 -0.008787423 -0.03534992\nst3 -0.03845952 -0.042944399  1.000000000 -0.009515269  0.04626172\nst4 -0.04575890 -0.008787423 -0.009515269  1.000000000  0.26083401\nst5 -0.11805906 -0.035349917  0.046261723  0.260834006  1.00000000\n\n\n\nspearman_cor_db &lt;- spearman_cor %&gt;% \n  as.data.frame() %&gt;% \n  rownames_to_column(\"var1\") %&gt;% \n  pivot_longer(all_of(stations), names_to = \"var2\", values_to = \"cor\") %&gt;% \n  filter(var1 &gt; var2)\n\n# check genres with the lowest correlations:\nspearman_cor_db %&gt;% \n  arrange(desc(cor)) %&gt;% \n  head()\n\n# A tibble: 6 × 3\n  var1  var2    cor\n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n1 st6   st5   0.306\n2 st8   st7   0.298\n3 st5   st4   0.261\n4 st5   st11  0.255\n5 st6   st11  0.156\n6 st4   st11  0.137\n\n\n\nsummary(spearman_cor_db$cor)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.118059 -0.009163  0.024514  0.042897  0.064182  0.305998 \n\n\nNext, we create a simulation using 100k sampling units from a multivariate Normal using these observed correlations and we map the cumulative distributions of these \\(N(0,1)\\) margins to the observed cumulative distributions.\n\nradio_sim_db_v1 &lt;- simulation_fc(\n    actual_distr = actual_distr,\n    cor_mat = spearman_cor,\n    sample_size = 100000\n)\n\nnames(radio_sim_db_v1) &lt;- stations\nradio_sim_db_v1[1:5, 1:5]\n\n  st1 st2 st3 st4 st5\n1   0   0   0   0   0\n2   0   0   0   0   0\n3   0   0   0   0   0\n4   0   0 180   0   0\n5   0   0 195   0 840\n\n\nCheck simulated reach% against actual reach%:\n\ndata.frame(sim = colSums(radio_sim_db_v1 &gt; 0) / nrow(radio_sim_db_v1)) %&gt;% \n  rownames_to_column(\"name\") %&gt;% \n  full_join(\n    actual_distr %&gt;% \n      filter(value == 0) %&gt;% \n      mutate(actual = 1-pc) %&gt;% \n      select(name, actual) ,\n      by = \"name\"\n  ) %&gt;% \n  mutate(diff = round(sim - actual, digits = 3))\n\n   name     sim  actual diff\n1   st1 0.16539 0.16539    0\n2   st2 0.01276 0.01276    0\n3   st3 0.32480 0.32480    0\n4   st4 0.04112 0.04112    0\n5   st5 0.22966 0.22966    0\n6   st6 0.04284 0.04284    0\n7   st7 0.10477 0.10477    0\n8   st8 0.02128 0.02128    0\n9   st9 0.04773 0.04773    0\n10 st10 0.00665 0.00665    0\n11 st11 0.02895 0.02895    0\n\n\nCheck simulated total hours against actual total hours:\n\ndb %&gt;% \n  pivot_longer(all_of(stations)) %&gt;% \n  group_by(name) %&gt;% \n  summarise(actual = round(sum(value/60))) %&gt;% \n  left_join(\n    radio_sim_db_v1 %&gt;% \n    pivot_longer(all_of(stations)) %&gt;% \n    group_by(name) %&gt;% \n    summarise(sim = round(sum(value/60))),\n    by = \"name\"\n  ) %&gt;% \n  mutate(diff = actual - sim)\n\n# A tibble: 11 × 4\n   name  actual    sim  diff\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 st1   103230 103230     0\n 2 st10    2946   2946     0\n 3 st11   14602  14602     0\n 4 st2     5887   5887     0\n 5 st3   382685 382685     0\n 6 st4    26262  26262     0\n 7 st5   255475 255475     0\n 8 st6    27703  27703     0\n 9 st7    70762  70762     0\n10 st8     6965   6965     0\n11 st9    44461  44461     0\n\n\nThe simulated dataset match the reach and total hours of the actual dataset for all stations.\n\n3.2.1.1 univariate comparison\nAs we have mapped each \\(N(0,1)\\) margin to the observed marginal distributions, there should be no difference between actual and simulated margins:\n\nrbind(\n  \n  db %&gt;% \n    pivot_longer(all_of(stations)) %&gt;% \n    select(-id) %&gt;% \n    mutate(source = \"actual\"),\n  \n  radio_sim_db_v1 %&gt;% \n    pivot_longer(all_of(stations)) %&gt;% \n    mutate(source = \"sim\")\n  \n) %&gt;% \n  filter(value &gt; 0, value &lt; 600) %&gt;% \n  ggplot(aes(x = value, col = source)) +\n  geom_density() +\n  facet_wrap(~name,  ncol = 3) +\n  labs(title = \"actual vs simulated genre distributions\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nAs expected, margins are an exact match (the actual and simulated densities can’t be distinguished in the above plot) so any subsequent calculations based on summing the values will be preserved.\n\n\n3.2.1.2 bivariate comparison\nTo start with, we can compare the Spearman correlations in the original and simulated datasets:\n\ncompare_corrs &lt;- rbind(\n  \n  # actual correlations:\n  spearman_cor %&gt;% \n    as.data.frame() %&gt;% \n    rownames_to_column(\"var1\") %&gt;% \n    pivot_longer(all_of(stations), names_to = \"var2\", values_to = \"cor\") %&gt;% \n    filter(var1 &gt; var2) %&gt;% \n    mutate(source = \"actual\"),\n  \n  # simulated transformed correlations:\n  cor(radio_sim_db_v1, method = \"spearman\") %&gt;% \n    as.data.frame() %&gt;% \n    rownames_to_column(\"var1\") %&gt;% \n    pivot_longer(all_of(stations), names_to = \"var2\", values_to = \"cor\") %&gt;% \n    filter(var1 &gt; var2) %&gt;% \n    mutate(source = \"sim\")\n) %&gt;% \n  pivot_wider(names_from = \"source\", values_from = \"cor\")\n\nhead(compare_corrs)\n\n# A tibble: 6 × 4\n  var1  var2    actual      sim\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 st2   st1    0.102    0.0241 \n2 st2   st10   0.0487   0.00269\n3 st2   st11   0.00275  0.00322\n4 st3   st1   -0.0385  -0.0205 \n5 st3   st2   -0.0429  -0.00666\n6 st3   st10  -0.0369  -0.00816\n\n\n\ncompare_corrs %&gt;% \n  ggplot(aes(x = actual, y = sim)) +\n  geom_point(col = \"maroon\") +\n  geom_abline(slope = 1,  col = \"grey\") +\n  expand_limits(x = c(-0.2, 0.3), y = c(-0.2, 0.3)) +\n  labs(\n    title = \"V1: genre pairwise Spearman correlations\",\n    x = \"actual correlations\", y = \"simulated correlations\"\n    )\n\n\n\n\n\n\n\n\nWe see that the simulated pairwise correlations have a much smaller span compared with original correlations.\nNext we can check the bivariate results for each pair of stations. To do this, we create 4 categories for each univariate:\n- cat0: non-users.\n- cat1: light users (first tertile based on non-null values).\n- cat2: medium users (second tertile based on non-null values).\n- cat3: heavy users (third tertile based on non-null values).\nThen we calculate the percentage reach for all pairs of genres and all categories: P(X = cat0 and Y = cat0) etc…\n\n# create all possible pairs of genres:\nmy_grid &lt;- expand.grid(\n  var1 = stations,\n  var2 = stations,\n  stringsAsFactors = FALSE\n) %&gt;% \n  arrange(var1, var2) %&gt;% \n  filter(var1 &lt; var2)\n\nresults_v1 &lt;- joint_reach_categories_fc(\n  my_grid = my_grid, \n  actual_db = db, \n  sim_db = radio_sim_db_v1,\n  my_source = \"sim_v1\"\n    )\n\nhead(results_v1)\n\n# A tibble: 6 × 7\n   cat0    cat1    cat2    cat3 source var1  var2 \n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n1 0.829 0.00033 0.00018 0.00012 actual st1   st10 \n2 0.829 0.00015 0.0001  0.00016 sim_v1 st1   st10 \n3 0.808 0.00057 0.00028 0.00015 actual st1   st11 \n4 0.810 0.0005  0.0006  0.00037 sim_v1 st1   st11 \n5 0.828 0.00106 0.00085 0.0008  actual st1   st2  \n6 0.825 0.00032 0.00034 0.00038 sim_v1 st1   st2  \n\n\nNote that as we have high levels of zeros, the remaining listening weight categories are actually really small.\n\ncats &lt;- paste0(\"cat\", 0:3)\nmy_cols &lt;- c(\"slateblue\", \"cornflowerblue\", \"royalblue1\",\"thistle\")\n\nfor(i in 1:length(cats)){\n  \n  p &lt;- results_v1 %&gt;% \n    select(all_of(c(\"source\", \"var1\", \"var2\", cats[i]))) %&gt;% \n    rename(cat = cats[i]) %&gt;% \n    pivot_wider(names_from = source, values_from = cat) %&gt;% \n    ggplot(aes(x = actual, y = sim_v1)) +\n    geom_point(col = my_cols[i]) +\n    geom_abline(slope = 1,  col = \"grey\") +\n    labs(title = paste0(cats[i], \" : pairwise reach\"))\n  \n  plot(p)\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen looking at the pairwise distributions we notice that:\n- the non-users of both genres (cat0) are well aligned with small differences.\n- the light-users of both genres (cat1) are systematically under-estimated.\nThere is quite a spread in the weight of listening categories but these are based on very small sample size, and what matters most are the combined zero categories.\n\n\n3.2.1.3 multivariate comparison\nWhat happens when we calculate the reach for 2, 3, 4… all genres? let’s start with the smallest genres and add the next bigger genre at each step.\n\nstation_ranking &lt;- db %&gt;% \n  pivot_longer(all_of(stations)) %&gt;% \n   group_by(name) %&gt;% \n   summarise(reach = sum(value &gt; 0) ) %&gt;%  \n   arrange(desc(reach))\n\nmy_stations &lt;- station_ranking %&gt;% head(2)\nmy_stations\n\n# A tibble: 2 × 2\n  name  reach\n  &lt;chr&gt; &lt;int&gt;\n1 st3   32480\n2 st5   22966\n\n\n\nmy_list &lt;- list()\n\nfor(i in 2:nrow(station_ranking)){\n  \n  my_stations &lt;- station_ranking %&gt;% head(i)\n  \n  actual_mat &lt;- db[, my_stations$name]\n  actual_comb_sum &lt;-  rowSums(actual_mat)\n  \n  sim_mat &lt;- radio_sim_db_v1[, my_stations$name]\n  sim_comb_sum &lt;-  rowSums(sim_mat)\n  \n  my_list[[i-1]] &lt;- data.frame(\n    genre_combination = paste(my_stations$name, collapse = \", \"),\n    n1 = ncol(actual_mat),\n    n2 = ncol(sim_mat),\n    actual_pc = sum(actual_comb_sum &gt; 0) / length(actual_comb_sum),\n    sim_pc = sum(sim_comb_sum &gt; 0) / length(sim_comb_sum)\n  )\n}\n\nmultivariate_v1 &lt;- bind_rows(my_list) \n\nmultivariate_v1 %&gt;% head()\n\n                  genre_combination n1 n2 actual_pc  sim_pc\n1                          st3, st5  2  2   0.46381 0.47550\n2                     st3, st5, st1  3  3   0.56756 0.57095\n3                st3, st5, st1, st7  4  4   0.59904 0.61112\n4           st3, st5, st1, st7, st9  5  5   0.60775 0.62575\n5      st3, st5, st1, st7, st9, st6  6  6   0.61062 0.63658\n6 st3, st5, st1, st7, st9, st6, st4  7  7   0.61525 0.64859\n\n\n\nmultivariate_v1 %&gt;% \n  ggplot(aes(x = actual_pc,  y = sim_pc)) +\n  geom_point(col = \"slateblue\") +\n  geom_abline(slope = 1,  col = \"grey\") +\n  scale_x_continuous(labels = scales::percent) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"V1: actual vs simulated combined reach\")\n\n\n\n\n\n\n\n\n=&gt; as we increase the number of stations past the first 3, the simulated result gets progressively higher than the actual result as we are accumlating error.\n\ntail(multivariate_v1, 1)\n\n                                         genre_combination n1 n2 actual_pc\n10 st3, st5, st1, st7, st9, st6, st4, st11, st8, st2, st10 11 11   0.62651\n    sim_pc\n10 0.66793\n\n\nWhen we combine the reach across all stations, the actual reach is 62.7% but the simulated reach is higher at 66.8%.\n\n\n\n3.2.2 using Tetrachoric correlation\nThe polychoric correlation is a technique for estimating the correlation between two hypothesised normally distributed continuous latent variables, from two observed ordinal variables. Tetrachoric correlation is a special case of the polychoric correlation applicable when both observed variables are dichotomous. (source: wikipedia).\nIn R, we can use the convenient function psych::phi2tetra to find the Tetrachoric correlation that corresponds to the combination of a “Phi coefficient”, i.e. the correlation between the two binary vectors, as well as their marginals.\nCalculate the tetrachoric correlation for all pairs of genres that include the zero value:\n\n# create all possible pairs of genres:\nall_pairs &lt;- expand.grid(\n  var1 = stations,\n  var2 = stations,\n  stringsAsFactors = FALSE\n) %&gt;% \n  arrange(var1, var2) %&gt;% \n  filter(var1 &lt; var2) %&gt;% \n  mutate(\n    phi_coef = NA,\n    tcc = NA\n    )\n\nfor(i in 1:nrow(all_pairs)){\n  \n  n &lt;- nrow(db)\n  \n  v1 &lt;- all_pairs$var1[i]\n  v2 &lt;- all_pairs$var2[i]\n  \n  # transform the 1st genre to binary:\n  x1 &lt;- db %&gt;% \n    select(!!sym(v1)) %&gt;% \n    rename(v1 = !!sym(v1)) %&gt;% \n    mutate(v1 = ifelse(v1 &gt; 0, 1, 0)) %&gt;% \n    pull(v1)\n  \n  # transform the 2nd genre to binary:\n  x2 &lt;- db %&gt;% \n    select(!!sym(v2)) %&gt;% \n    rename(v2 = !!sym(v2)) %&gt;% \n    mutate(v2 = ifelse(v2 &gt; 0, 1, 0)) %&gt;% \n    pull(v2)\n  \n  # calculate the phi coefficient:\n  all_pairs$phi_coef[i] &lt;- cor(x1, x2)\n  \n  # convert to tetrachoric correlation:\n  all_pairs$tcc[i] &lt;- phi2tetra(\n    ph = cor(x1, x2), \n    m = c(mean(x1), mean(x2))\n    )\n  \n}\n\nall_pairs %&gt;% \n  arrange(desc(tcc)) %&gt;% \n  head()\n\n  var1 var2  phi_coef       tcc\n1  st7  st8 0.2982160 0.7090633\n2  st5  st6 0.2977637 0.6823495\n3 st11  st5 0.2345534 0.6246644\n4  st4  st5 0.2533079 0.5997845\n5 st11  st6 0.1552139 0.4537150\n6 st11  st4 0.1369527 0.4209029\n\n\n\nsummary(all_pairs)\n\n     var1               var2              phi_coef              tcc          \n Length:55          Length:55          Min.   :-0.107399   Min.   :-0.24995  \n Class :character   Class :character   1st Qu.:-0.006035   1st Qu.:-0.03358  \n Mode  :character   Mode  :character   Median : 0.025025   Median : 0.10832  \n                                       Mean   : 0.045090   Mean   : 0.12410  \n                                       3rd Qu.: 0.066445   3rd Qu.: 0.25108  \n                                       Max.   : 0.298216   Max.   : 0.70906  \n\n\n\nall_pairs %&gt;% \n  ggplot(aes(x = phi_coef, y = tcc)) +\n  geom_point(col = \"darkorange\") +\n  geom_abline(slope = 1,  col = \"grey\") +\n  scale_x_continuous(labels = scales::percent) +\n  scale_y_continuous(labels = scales::percent) +\n  expand_limits(x = c(-1, 1), y = c(-1,1)) +\n  labs(title = \"V1: actual vs simulated combined reach\")\n\n\n\n\n\n\n\n\nThe Tetrachoric correlations have a larger span than the Phi coefficients.\nTurn into a matrix:\n\n# add 1 for the ycc between same station pairs:\ndiag &lt;- data.frame(\n  var1 = stations,\n  var2 = stations,\n  tcc = 1\n)\n\ntcc_df &lt;- rbind(\n  # upper triangle:\n  all_pairs %&gt;% \n    select(-phi_coef),\n  # diagonal of 1s:\n  diag,\n  # lower triangle obtained by inversing genre1 and genre2:\n  all_pairs %&gt;% \n    select(-phi_coef) %&gt;% \n    rename(\n      var2 = var1,\n      var1 = var2\n    ) %&gt;% \n    select(var1, var2, tcc)\n) %&gt;% \n  arrange(var1, var2)\n\ntcc_cor &lt;- as.matrix(\n  tcc_df %&gt;% \n    # make sure the variables are in the correct order:\n    mutate(\n      var1 = factor(var1, levels = stations),\n      var2 = factor(var2, levels = stations)\n      ) %&gt;% \n    arrange(var1, var2) %&gt;% \n    pivot_wider(names_from = var2, values_from = tcc) %&gt;% \n    select(-var1)\n)\n\ntcc_cor[1:5, 1:5]\n\n             st1        st2         st3         st4        st5\n[1,]  1.00000000  0.3762087 -0.03229833 -0.17018040 -0.2445209\n[2,]  0.37620871  1.0000000 -0.20342287 -0.07833120 -0.1923967\n[3,] -0.03229833 -0.2034229  1.00000000  0.01393292  0.1434055\n[4,] -0.17018040 -0.0783312  0.01393292  1.00000000  0.5997845\n[5,] -0.24452090 -0.1923967  0.14340546  0.59978449  1.0000000\n\n\nCheck that this is a valid correlation matrix, i.e. its eigen values are non-negative:\n\neigen(tcc_cor)$values\n\n [1] 3.2053364 1.6814288 1.4463485 1.0975162 0.7858067 0.6941503 0.6168792\n [8] 0.5146659 0.4935429 0.2795524 0.1847726\n\n\nIf they are not all positive, we can return the nearest positive definite matrix with Matrix::nearPD:\n\nif( sum(eigen(tcc_cor)$values &gt; 0) &lt; nrow(tcc_cor)){\n    \n    tcc_cor &lt;- as.matrix(nearPD(tcc_cor, corr = TRUE)$mat)\n  \n  }\n\ntcc_cor[1:5, 1:5]\n\n             st1        st2         st3         st4        st5\n[1,]  1.00000000  0.3762087 -0.03229833 -0.17018040 -0.2445209\n[2,]  0.37620871  1.0000000 -0.20342287 -0.07833120 -0.1923967\n[3,] -0.03229833 -0.2034229  1.00000000  0.01393292  0.1434055\n[4,] -0.17018040 -0.0783312  0.01393292  1.00000000  0.5997845\n[5,] -0.24452090 -0.1923967  0.14340546  0.59978449  1.0000000\n\n\nThen generate the MVN with the tccs instead of the correlations:\n\nradio_sim_db_v2 &lt;- simulation_fc(\n    actual_distr = actual_distr,\n    cor_mat = tcc_cor,\n    sample_size = 100000\n)\n\nnames(radio_sim_db_v2) &lt;- stations\nradio_sim_db_v2[1:5, 1:5]\n\n  st1 st2 st3 st4 st5\n1   0   0   0   0  45\n2   0   0   0   0   0\n3  90   0 255   0   0\n4   0   0   0   0   0\n5   0   0 135   0   0\n\n\nCheck simulated reach% against actual reach%:\n\ndata.frame(sim = colSums(radio_sim_db_v2 &gt; 0) / nrow(radio_sim_db_v2)) %&gt;% \n  rownames_to_column(\"name\") %&gt;% \n  full_join(\n    actual_distr %&gt;% \n      filter(value == 0) %&gt;% \n      mutate(actual = 1-pc) %&gt;% \n      select(name, actual) ,\n      by = \"name\"\n  ) %&gt;% \n  mutate(diff = round(sim - actual, digits = 3))\n\n   name     sim  actual diff\n1   st1 0.16539 0.16539    0\n2   st2 0.01276 0.01276    0\n3   st3 0.32480 0.32480    0\n4   st4 0.04112 0.04112    0\n5   st5 0.22966 0.22966    0\n6   st6 0.04284 0.04284    0\n7   st7 0.10477 0.10477    0\n8   st8 0.02128 0.02128    0\n9   st9 0.04773 0.04773    0\n10 st10 0.00665 0.00665    0\n11 st11 0.02895 0.02895    0\n\n\nCheck simulated total hours against actual total hours:\n\ndb %&gt;% \n  pivot_longer(all_of(stations)) %&gt;% \n  group_by(name) %&gt;% \n  summarise(actual = round(sum(value/60))) %&gt;% \n  left_join(\n    radio_sim_db_v2 %&gt;% \n    pivot_longer(all_of(stations)) %&gt;% \n    group_by(name) %&gt;% \n    summarise(sim = round(sum(value/60))),\n    by = \"name\"\n  ) %&gt;% \n  mutate(diff = actual - sim)\n\n# A tibble: 11 × 4\n   name  actual    sim  diff\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 st1   103230 103230     0\n 2 st10    2946   2946     0\n 3 st11   14602  14602     0\n 4 st2     5887   5887     0\n 5 st3   382685 382685     0\n 6 st4    26262  26262     0\n 7 st5   255475 255475     0\n 8 st6    27703  27703     0\n 9 st7    70762  70762     0\n10 st8     6965   6965     0\n11 st9    44461  44461     0\n\n\nThe simulated dataset match the reach and total hours of the actual dataset for all stations.\n\n3.2.2.1 univariate comparison\nAs we have mapped each \\(N(0,1)\\) margin to the observed marginal distributions, there should be no difference between actual and simulated margins:\n\nrbind(\n  \n  db %&gt;% \n    pivot_longer(all_of(stations)) %&gt;% \n    select(-id) %&gt;% \n    mutate(source = \"actual\"),\n  \n  radio_sim_db_v2 %&gt;% \n    pivot_longer(all_of(stations)) %&gt;% \n    mutate(source = \"sim\")\n  \n) %&gt;% \n  filter(value &gt; 0, value &lt; 600) %&gt;% \n  ggplot(aes(x = value, col = source)) +\n  geom_density() +\n  facet_wrap(~name, ncol = 3) +\n  labs(title = \"V2: actual vs simulated genre distributions\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nAs expected, margins are an exact match so any subsequent calculations based on summing the values will be preserved.\n\n\n3.2.2.2 bivariate comparison\nTo start with, we can compare the Spearman correlations in the original and simulated datasets:\n\ncompare_corrs &lt;- rbind(\n  \n  # actual correlations:\n  spearman_cor %&gt;% \n    as.data.frame() %&gt;% \n    rownames_to_column(\"var1\") %&gt;% \n    pivot_longer(all_of(stations), names_to = \"var2\", values_to = \"cor\") %&gt;% \n    filter(var1 &gt; var2) %&gt;% \n    mutate(source = \"actual\"),\n  \n  # simulated transformed correlations:\n  cor(radio_sim_db_v2, method = \"spearman\") %&gt;% \n    as.data.frame() %&gt;% \n    rownames_to_column(\"var1\") %&gt;% \n    pivot_longer(all_of(stations), names_to = \"var2\", values_to = \"cor\") %&gt;% \n    filter(var1 &gt; var2) %&gt;% \n    mutate(source = \"sim\")\n) %&gt;% \n  pivot_wider(names_from = \"source\", values_from = \"cor\")\n\nhead(compare_corrs)\n\n# A tibble: 6 × 4\n  var1  var2    actual      sim\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 st2   st1    0.102    0.107  \n2 st2   st10   0.0487   0.0500 \n3 st2   st11   0.00275  0.00425\n4 st3   st1   -0.0385  -0.0131 \n5 st3   st2   -0.0429  -0.0444 \n6 st3   st10  -0.0369  -0.0334 \n\n\n\ncompare_corrs %&gt;% \n  ggplot(aes(x = actual, y = sim)) +\n  geom_point(col = \"maroon\") +\n  geom_abline(slope = 1,  col = \"grey\") +\n  expand_limits(x = c(-0.2, 0.3), y = c(-0.2, 0.3)) +\n  labs(\n    title = \"V2: genre pairwise Spearman correlations\",\n    x = \"actual correlations\", y = \"simulated correlations\"\n    )\n\n\n\n\n\n\n\n\nPreviously we saw that when using the Spearman correlations the resulting simulated correlations had a much smaller span compared with original correlations. When we switch to the Tetrachoric correlation, the resulting simulated correlations are much closer to the actual ones.\nNext we can check the bivariate results for each pair of stations. To do this, we create 4 categories for each univariate:\n- cat0: non-users.\n- cat1: light users (first tertile based on non-null values).\n- cat2: medium users (second tertile based on non-null values).\n- cat3: heavy users (third tertile based on non-null values).\nThen we calculate the percentage reach for all pairs of genres and all categories: P(X = cat0 and Y = cat0) etc…\n\n# create all possible pairs of genres:\nmy_grid &lt;- expand.grid(\n  var1 = stations,\n  var2 = stations,\n  stringsAsFactors = FALSE\n) %&gt;% \n  arrange(var1, var2) %&gt;% \n  filter(var1 &lt; var2)\n\nresults_v2 &lt;- joint_reach_categories_fc(\n  my_grid = my_grid, \n  actual_db = db, \n  sim_db = radio_sim_db_v2,\n  my_source = \"sim_v2\"\n    )\n\nhead(results_v2)\n\n# A tibble: 6 × 7\n   cat0    cat1    cat2    cat3 source var1  var2 \n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n1 0.829 0.00033 0.00018 0.00012 actual st1   st10 \n2 0.829 0.00019 0.00012 0.0002  sim_v2 st1   st10 \n3 0.808 0.00057 0.00028 0.00015 actual st1   st11 \n4 0.808 0.00027 0.00025 0.00022 sim_v2 st1   st11 \n5 0.828 0.00106 0.00085 0.0008  actual st1   st2  \n6 0.828 0.00049 0.00041 0.00113 sim_v2 st1   st2  \n\n\n\ncats &lt;- paste0(\"cat\", 0:3)\nmy_cols &lt;- c(\"slateblue\", \"cornflowerblue\", \"royalblue1\",\"thistle\")\n\nfor(i in 1:length(cats)){\n  \n  p &lt;- results_v2 %&gt;% \n    select(all_of(c(\"source\", \"var1\", \"var2\", cats[i]))) %&gt;% \n    rename(cat = cats[i]) %&gt;% \n    pivot_wider(names_from = source, values_from = cat) %&gt;% \n    ggplot(aes(x = actual, y = sim_v2)) +\n    geom_point(col = my_cols[i]) +\n    geom_abline(slope = 1,  col = \"grey\") +\n    labs(title = paste0(cats[i], \" : pairwise reach\"))\n  \n  plot(p)\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n=&gt; when looking at the pairwise distributions we notice that:\n- the non-users of both stations (cat0) are well aligned with almost zero differences.\n- the light-users of both stations (cat1) are systematically under-estimated but this is based on very small sample sizes.\n\n\n3.2.2.3 multivariate comparison\nWhat happens when we calculate the reach for 2, 3, 4… all genres? let’s start with the smallest genres and add the next bigger genre at each step.\n\nstation_ranking &lt;- db %&gt;% \n  pivot_longer(all_of(stations)) %&gt;% \n   group_by(name) %&gt;% \n   summarise(reach = sum(value &gt; 0) ) %&gt;%  \n   arrange(desc(reach))\n\nmy_stations &lt;- station_ranking %&gt;% head(2)\nmy_stations\n\n# A tibble: 2 × 2\n  name  reach\n  &lt;chr&gt; &lt;int&gt;\n1 st3   32480\n2 st5   22966\n\n\n\nmy_list &lt;- list()\n\nfor(i in 2:nrow(station_ranking)){\n  \n  my_stations &lt;- station_ranking %&gt;% head(i)\n  \n  actual_mat &lt;- db[, my_stations$name]\n  actual_comb_sum &lt;-  rowSums(actual_mat)\n  \n  sim_mat &lt;- radio_sim_db_v2[, my_stations$name]\n  sim_comb_sum &lt;-  rowSums(sim_mat)\n  \n  my_list[[i-1]] &lt;- data.frame(\n    genre_combination = paste(my_stations$name, collapse = \", \"),\n    n1 = ncol(actual_mat),\n    n2 = ncol(sim_mat),\n    actual_pc = sum(actual_comb_sum &gt; 0) / length(actual_comb_sum),\n    sim_pc = sum(sim_comb_sum &gt; 0) / length(sim_comb_sum)\n  )\n}\n\nmultivariate_v2 &lt;- bind_rows(my_list) \n\nmultivariate_v2 %&gt;% head()\n\n                  genre_combination n1 n2 actual_pc  sim_pc\n1                          st3, st5  2  2   0.46381 0.46432\n2                     st3, st5, st1  3  3   0.56756 0.56544\n3                st3, st5, st1, st7  4  4   0.59904 0.59791\n4           st3, st5, st1, st7, st9  5  5   0.60775 0.60753\n5      st3, st5, st1, st7, st9, st6  6  6   0.61062 0.61133\n6 st3, st5, st1, st7, st9, st6, st4  7  7   0.61525 0.61713\n\n\n\nmultivariate_v2 %&gt;% \n  ggplot(aes(x = actual_pc,  y = sim_pc)) +\n  geom_point(col = \"slateblue\") +\n  geom_abline(slope = 1,  col = \"grey\") +\n  scale_x_continuous(labels = scales::percent) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"V2: actual vs simulated combined reach\")\n\n\n\n\n\n\n\n\nAs we increase the number of stations the simulated combined reach remains very close to the actual combined reach which is a much better result than when using the Speaerman correlation.\n\ntail(multivariate_v2, 1)\n\n                                         genre_combination n1 n2 actual_pc\n10 st3, st5, st1, st7, st9, st6, st4, st11, st8, st2, st10 11 11   0.62651\n    sim_pc\n10 0.62896\n\n\nWhen we combine the reach across all stations, the actual reach is 62.7% and now the simulated reach is 62.9% which is much better than when we used Spearman correlation.\nNote that although this analysis is based on a synthetic dataset, the exact same results were found with the original RAJAR data.\n\n\n\n3.2.3 conclusion\nStarting with a random sample generated from a multivariate normal using the Tetrachoric correlations between all stations, each resulting margin was then transformed to match the observed time spent distribution for each station using their empirical quantiles.\nThe resulting simulated dataset preserved all pairs joint reach but also the combined reach from any number of stations. It also preserved the total time spent for all stations. The joint distribution of light / medium / heavy listening was reasonable but is based on much smaller sample sizes.\nThe main objectives of media planning are to preserve combined reach and total volumes. When using data fusion, any resulting differences in the currencies are corrected through calibration, however the same cannot be done for the reach. Using copulas, there is no need for calibration and the combined reach is preserved."
  },
  {
    "objectID": "posts/cross_media_with_copulas/index.html#application-to-measuring-local-cross-media-usage",
    "href": "posts/cross_media_with_copulas/index.html#application-to-measuring-local-cross-media-usage",
    "title": "Estimating Cross Media Usage Using Copulas",
    "section": "3.3 application to measuring local cross media usage",
    "text": "3.3 application to measuring local cross media usage\nIn practice of course we don’t have a single source currency multi-media dataset but we may still have some relevant cross-media information that we want to incorporate.\nWhen trying to join distributions from different data sources, the easiest approach is to assume they are independent, then the joint zero is just the product of the individual zeros: \\(P(X_1 = 0, X_2 = 0) = P(X_1 = 0) P(X_2 = 0)\\). In media planning, this is referred to as the Sainsbury formula. However we may have evidence from a separate study that two media are in fact dependent and the problem is then to find a way of incorporating this knowledge to provide a better estimate of cross media reach.\nFor example in the UK, to measure joint media reach at a local level, we may still be able to use the currencies for each media as long as they provide sufficient data points in each local area. To join them, we can use information from a separate cross media data source that measures the same media but at a higher relevant geographical level. Whilst not perfect, this gives us a much more realistic cross media dependence structure than the hypothesis of independence.\nThis approach is currently used to estimate the BBC’s cross media reach for local content in each local area, as defined by the local radio stations measurement footprint."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christel Lacaze Swift",
    "section": "",
    "text": "Estimating Cross Media Usage Using Copulas\n\n\nA Practical Guide\n\n\n\nmedia measurement\n\n\nstatistics\n\n\ncopulas\n\n\n\nA practical guide to estimating cross-media audience usage with copulas as an alternative to traditional data fusion approaches. \n\n\n\n\n\nJul 18, 2025\n\n\nChristel Lacaze Swift\n\n\n\n\n\n\nNo matching items"
  }
]